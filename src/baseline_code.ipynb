{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 중에서 안깔린 라이브러리가 있을 수 있음\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../data/\"\n",
    "\n",
    "train = pd.read_csv(path+\"train.csv\").assign(_type=\"train\")\n",
    "test = pd.read_csv(path+\"test.csv\").assign(_type=\"test\")\n",
    "submission = pd.read_csv(path+\"test.csv\")\n",
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 원래 베이스라인 코드 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "# file_names: List[str] = [\n",
    "#     f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "# ]\n",
    "\n",
    "# # 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "# file_dict: Dict[str, pd.DataFrame] = {\n",
    "#     f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "# }\n",
    "\n",
    "# for _file_name, _df in tqdm(file_dict.items()):\n",
    "#     # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "#     _rename_rule = {\n",
    "#         col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "#         for col in _df.columns\n",
    "#     }\n",
    "#     _df = _df.rename(_rename_rule, axis=1)\n",
    "#     df = df.merge(_df, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 밑의 한 셀이 바꾼 부분 (이 바꾼 부분을 전제로 뒤의 코드가 실행됨.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOURLY_NETWORK-DATA_BLOCK-BYTES.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_UTXO-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_DIFFICULTY.csv',\n",
       " 'HOURLY_NETWORK-DATA_ADDRESSES-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_HASHRATE.csv',\n",
       " 'HOURLY_NETWORK-DATA_TOKENS-TRANSFERRED.csv',\n",
       " 'HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCKREWARD.csv',\n",
       " 'HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE.csv',\n",
       " 'HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES.csv',\n",
       " 'HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES-TRANSACTION.csv',\n",
       " 'HOURLY_NETWORK-DATA_SUPPLY.csv',\n",
       " 'HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-INTERVAL.csv',\n",
       " 'HOURLY_NETWORK-DATA_VELOCITY.csv',\n",
       " 'HOURLY_NETWORK-DATA_TRANSACTIONS-COUNT.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 27.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_로 시작하는 모든 csv 파일 불러오기\n",
    "prefixes = [\"HOURLY_NETWORK-DATA_\",\n",
    "            \"HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD\",\n",
    "            \"HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX\",\n",
    "            \"HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE\",\n",
    "            \"HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL\",\n",
    "            \"HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL\",\n",
    "            \"HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE\"]\n",
    "\n",
    "file_names = [\n",
    "    f for f in os.listdir(path)\n",
    "    if any(f.startswith(prefix) for prefix in prefixes) and f.endswith(\".csv\")\n",
    "]\n",
    "display(file_names)\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(path+f) for f in file_names\n",
    "}\n",
    "\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    _rename_rule = {\n",
    "        col: f\"{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결측치 (아거도 추가된 부분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "volume                         2792\n",
       "close                          2792\n",
       "block_bytes                      54\n",
       "funding_rates                    32\n",
       "taker_buy_sell_ratio             31\n",
       "taker_sell_ratio                 31\n",
       "taker_buy_ratio                  31\n",
       "taker_sell_volume                31\n",
       "taker_buy_volume                 31\n",
       "supply_new                       28\n",
       "supply_total                     28\n",
       "transactions_count_mean          28\n",
       "fees_block_mean_usd              24\n",
       "fees_transaction_mean            24\n",
       "fees_transaction_mean_usd        24\n",
       "tokens_transferred_mean          24\n",
       "fees_reward_percent              24\n",
       "block_interval                   24\n",
       "difficulty                       24\n",
       "fees_block_mean                  24\n",
       "coinbase_premium_gap              6\n",
       "coinbase_premium_index            6\n",
       "transactions_count_total          4\n",
       "open_interest                     4\n",
       "block_count                       4\n",
       "utxo_count                        1\n",
       "addresses_count_active            0\n",
       "velocity_supply_total             0\n",
       "addresses_count_sender            0\n",
       "fees_transaction_median_usd       0\n",
       "fees_transaction_median           0\n",
       "blockreward                       0\n",
       "short_liquidations_usd            0\n",
       "blockreward_usd                   0\n",
       "short_liquidations                0\n",
       "long_liquidations                 0\n",
       "fees_total_usd                    0\n",
       "fees_total                        0\n",
       "addresses_count_receiver          0\n",
       "hashrate                          0\n",
       "tokens_transferred_total          0\n",
       "tokens_transferred_median         0\n",
       "long_liquidations_usd             0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "volume                         0.241690\n",
       "close                          0.241690\n",
       "block_bytes                    0.004675\n",
       "funding_rates                  0.002770\n",
       "taker_buy_sell_ratio           0.002684\n",
       "taker_sell_ratio               0.002684\n",
       "taker_buy_ratio                0.002684\n",
       "taker_sell_volume              0.002684\n",
       "taker_buy_volume               0.002684\n",
       "supply_new                     0.002424\n",
       "supply_total                   0.002424\n",
       "transactions_count_mean        0.002424\n",
       "fees_block_mean_usd            0.002078\n",
       "fees_transaction_mean          0.002078\n",
       "fees_transaction_mean_usd      0.002078\n",
       "tokens_transferred_mean        0.002078\n",
       "fees_reward_percent            0.002078\n",
       "block_interval                 0.002078\n",
       "difficulty                     0.002078\n",
       "fees_block_mean                0.002078\n",
       "coinbase_premium_gap           0.000519\n",
       "coinbase_premium_index         0.000519\n",
       "transactions_count_total       0.000346\n",
       "open_interest                  0.000346\n",
       "block_count                    0.000346\n",
       "utxo_count                     0.000087\n",
       "addresses_count_active         0.000000\n",
       "velocity_supply_total          0.000000\n",
       "addresses_count_sender         0.000000\n",
       "fees_transaction_median_usd    0.000000\n",
       "fees_transaction_median        0.000000\n",
       "blockreward                    0.000000\n",
       "short_liquidations_usd         0.000000\n",
       "blockreward_usd                0.000000\n",
       "short_liquidations             0.000000\n",
       "long_liquidations              0.000000\n",
       "fees_total_usd                 0.000000\n",
       "fees_total                     0.000000\n",
       "addresses_count_receiver       0.000000\n",
       "hashrate                       0.000000\n",
       "tokens_transferred_total       0.000000\n",
       "tokens_transferred_median      0.000000\n",
       "long_liquidations_usd          0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_new = df.drop([\"ID\", \"target\", \"_type\"], axis = 1)\n",
    "\n",
    "# 결측치 확인\n",
    "missing_data = df_new.isnull().sum()\n",
    "\n",
    "# 결측치 비율 확인\n",
    "missing_ratio = df_new.isnull().mean()\n",
    "\n",
    "\n",
    "display(missing_data.sort_values(ascending=False))\n",
    "display(missing_ratio.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_bytes                    0\n",
      "block_count                    0\n",
      "utxo_count                     0\n",
      "difficulty                     0\n",
      "addresses_count_active         0\n",
      "addresses_count_sender         0\n",
      "addresses_count_receiver       0\n",
      "hashrate                       0\n",
      "tokens_transferred_total       0\n",
      "tokens_transferred_mean        0\n",
      "tokens_transferred_median      0\n",
      "open_interest                  0\n",
      "blockreward                    0\n",
      "blockreward_usd                0\n",
      "funding_rates                  0\n",
      "taker_buy_volume               0\n",
      "taker_sell_volume              0\n",
      "taker_buy_ratio                0\n",
      "taker_sell_ratio               0\n",
      "taker_buy_sell_ratio           0\n",
      "fees_block_mean                0\n",
      "fees_block_mean_usd            0\n",
      "fees_total                     0\n",
      "fees_total_usd                 0\n",
      "fees_reward_percent            0\n",
      "long_liquidations              0\n",
      "short_liquidations             0\n",
      "long_liquidations_usd          0\n",
      "short_liquidations_usd         0\n",
      "coinbase_premium_gap           0\n",
      "coinbase_premium_index         0\n",
      "fees_transaction_mean          0\n",
      "fees_transaction_mean_usd      0\n",
      "fees_transaction_median        0\n",
      "fees_transaction_median_usd    0\n",
      "supply_total                   0\n",
      "supply_new                     0\n",
      "close                          0\n",
      "volume                         0\n",
      "block_interval                 0\n",
      "velocity_supply_total          0\n",
      "transactions_count_total       0\n",
      "transactions_count_mean        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. 결측치 처리 - 앞/뒤 보간법 (forward/backward fill)\n",
    "\n",
    "# df_ffill = df_new.fillna(method='ffill')  # 앞의 값을 사용하여 결측치 채움\n",
    "# df_bfill = df_new.fillna(method='bfill')  # 뒤의 값을 사용하여 결측치 채움\n",
    "\n",
    "# 2. 결측치 처리 - 선형 보간법 (linear interpolation)\n",
    "df_interpolated = df_new.interpolate(method='linear')\n",
    "\n",
    "# 3. 결측치 처리 - 평균값으로 대체\n",
    "# df_mean_filled = df_new.fillna(eda_df.mean())\n",
    "\n",
    "df.update(df_interpolated)\n",
    "\n",
    "# 결측치 처리 후 확인\n",
    "print(df_interpolated.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 데이터를 로드할 때, (제공 받은)베이스라인 코드에서와 같이 거래소와 암호화폐 종류를 불러오도록 설정을 했다면\n",
    "\n",
    "아래 코드는 작동하지 않음\n",
    "\n",
    "거래소와 암호화폐 종류를 고려하지 않은 코드임을 유의.\n",
    "\n",
    "(수정을 해야할 것 같음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 독립변수에서 내가 사용할 독립변수 고르기\n",
    "\n",
    "\n",
    "# 전체 feature\n",
    "features = df.columns\n",
    "# 내가 쓸 feature\n",
    "fe = ['ID', 'target', '_type', 'funding_rates', 'open_interest',\n",
    "       'taker_buy_volume', 'taker_sell_volume', 'taker_buy_ratio',\n",
    "       'taker_sell_ratio', 'taker_buy_sell_ratio', 'difficulty',\n",
    "       'transactions_count_total', 'transactions_count_mean', 'block_count',\n",
    "       'fees_transaction_mean', 'fees_transaction_mean_usd',\n",
    "       'fees_transaction_median', 'fees_transaction_median_usd',\n",
    "       'fees_block_mean', 'fees_block_mean_usd', 'fees_total',\n",
    "       'fees_total_usd', 'fees_reward_percent', 'hashrate', 'utxo_count',\n",
    "       'tokens_transferred_total', 'tokens_transferred_mean',\n",
    "       'tokens_transferred_median', 'block_interval', 'velocity_supply_total',\n",
    "       'supply_total', 'supply_new', 'addresses_count_active',\n",
    "       'addresses_count_sender', 'addresses_count_receiver',\n",
    "       'blockreward', 'blockreward_usd', 'buy_sell_volume_ratio']\n",
    "\n",
    "# new_features에는 내가 사용하지 않을 feature들이 담기고 나중에 drop됨. 때문에, 이름이 좀 잘못된 듯\n",
    "new_features = [x for x in features if x not in fe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"taker_buy_volume\"] / (df[\"taker_sell_volume\"] + 1),\n",
    ")\n",
    "\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
    "conti_cols = [_ for _ in (df.columns)[3:] if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\",\n",
    "    \"liquidation_diff\",\n",
    "    \"liquidation_usd_diff\",\n",
    "    \"volume_diff\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 🔥 피처엔지니어링 함수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 관련 생성\n",
    "def make_date_features(df: pd.DataFrame, date_column: str) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    입력된 데이터프레임의 특정 날짜 열을 기준으로 연도, 월, 주, 요일, 시간을 추출하여 새로운 피처로 추가.\n",
    "    Args:\n",
    "        df (pd.DataFrame): 날짜 컬럼을 포함하는 데이터프레임.\n",
    "        date_column (str): 날짜 정보가 담긴 열 이름.\n",
    "    Returns:\n",
    "        pd.DataFrame: 날짜 관련 피처가 추가된 데이터프레임 반환.\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df['year'] = df[date_column].dt.year  # 연도\n",
    "    df['month'] = df[date_column].dt.month  # 월 \n",
    "    df['week'] = df[date_column].dt.isocalendar().week  # 주\n",
    "    df['day_of_week'] = df[date_column].dt.dayofweek  # 요일 \n",
    "    df['hour'] = df[date_column].dt.hour  # 시간 \n",
    "    date_columns = [date_column, 'year', 'month', 'week', 'day_of_week', 'hour']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변동성, 차분 피처 생성 함수 \n",
    "def make_diff_change_feature(\n",
    "    df: pd.DataFrame, \n",
    "    columns_list: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    주어진 변수들에 대한 변동성, 차분 피처를 생성하고, 결측값을 처리하는 함수.\n",
    "    Args:\n",
    "        df (pd.DataFrame): 데이터를 포함한 데이터프레임.\n",
    "        columns_list (List[str]): 변동성과 차분 피처를 추가하고자 하는 열 이름 목록.\n",
    "    Returns:\n",
    "        pd.DataFrame: 변동성과 차분 피처가 추가된 데이터프레임.\n",
    "    \"\"\"  \n",
    "    new_features = {}  # 새로 추가할 열을 저장할 딕셔너리\n",
    "    for col in columns_list:\n",
    "        if col in df.columns:\n",
    "            pct_change_col = f'{col}_pct_change'\n",
    "            diff_col = f'{col}_diff'\n",
    "            new_features[pct_change_col] = df[col].pct_change(fill_method=None)\n",
    "            new_features[diff_col] = df[col].diff()\n",
    "        else:\n",
    "            print(f\"Error: Cannot find '{col}' column\")\n",
    "    new_features_df = pd.DataFrame(new_features, index=df.index) # 새 피처를 데이터프레임으로\n",
    "    new_features_df = new_features_df.ffill().bfill()  # 결측값 처리 (고정: ffill, bfill)\n",
    "    df = pd.concat([df, new_features_df], axis=1)  # 기존 데이터프레임에 추가\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 롱/숏 비율\n",
    "def make_longshort_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_col: str, \n",
    "    short_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    롱/숏 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_col (str): 롱(liquidations) 데이터가 저장된 열 이름\n",
    "        short_col (str): 숏(liquidations) 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 롱/숏 비율이 저장된 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['long_short_ratio'] = df[long_col] / (df[short_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청산/거래량 비율\n",
    "def make_liquidation_to_volume_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_col: str, \n",
    "    short_col: str, \n",
    "    buy_volume_col: str, \n",
    "    sell_volume_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    청산/거래량 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_col (str): 롱(liquidations) 데이터가 저장된 열 이름\n",
    "        short_col (str): 숏(liquidations) 데이터가 저장된 열 이름\n",
    "        buy_volume_col (str): 매수 거래량이 저장된 열 이름\n",
    "        sell_volume_col (str): 매도 거래량이 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 청산/거래량 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['liquidation_to_volume_ratio'] = (\n",
    "        (df[long_col] + df[short_col]) / \n",
    "        (df[buy_volume_col] + df[sell_volume_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청산된 USD 롱/숏 비율\n",
    "def make_liquidation_usd_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_usd_col: str, \n",
    "    short_usd_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    청산된 USD 롱/숏 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_usd_col (str): 롱(liquidations) USD 데이터가 저장된 열 이름\n",
    "        short_usd_col (str): 숏(liquidations) USD 데이터가 저장된 열 이름 \n",
    "    Returns:\n",
    "        pd.DataFrame: 청산된 USD 롱/숏 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['liquidation_usd_ratio'] = df[long_usd_col] / (df[short_usd_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 펀딩 비율과 롱/숏 포지션 차이 곱\n",
    "def make_funding_rate_position_change_feature(\n",
    "    df: pd.DataFrame, \n",
    "    funding_rate_col: str, \n",
    "    long_liquidations_col: str, \n",
    "    short_liquidations_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    펀딩 비율과 롱/숏 포지션 차이를 곱하여 포지션 변화를 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        funding_rate_col (str): 펀딩 비율 데이터가 저장된 열 이름\n",
    "        long_liquidations_col (str): 롱 청산 데이터가 저장된 열 이름\n",
    "        short_liquidations_col (str): 숏 청산 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 펀딩 비율에 따른 포지션 변화를 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['funding_rate_position_change'] = df[funding_rate_col] * (\n",
    "        df[long_liquidations_col] - df[short_liquidations_col]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프리미엄 갭과 프리미엄 인덱스의 차이\n",
    "def make_premium_diff_feature(\n",
    "    df: pd.DataFrame, \n",
    "    premium_gap_col: str, \n",
    "    premium_index_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    프리미엄 갭과 프리미엄 인덱스의 차이를 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        premium_gap_col (str): 프리미엄 갭 데이터가 저장된 열 이름\n",
    "        premium_index_col (str): 프리미엄 인덱스 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 프리미엄 갭과 프리미엄 인덱스의 차이를 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['premium_diff'] = df[premium_gap_col] - df[premium_index_col]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해시레이트와 난이도 간의 비율\n",
    "def make_hashrate_to_difficulty_feature(\n",
    "    df: pd.DataFrame, \n",
    "    hashrate_col: str, \n",
    "    difficulty_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    해시레이트와 난이도 간의 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        hashrate_col (str): 해시레이트 데이터가 저장된 열 이름\n",
    "        difficulty_col (str): 난이도 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 해시레이트와 난이도 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['hashrate_to_difficulty'] = df[hashrate_col] / (df[difficulty_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공급 변화율\n",
    "def make_supply_change_rate_feature(\n",
    "    df: pd.DataFrame, \n",
    "    new_supply_col: str, \n",
    "    total_supply_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    공급 변화율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        new_supply_col (str): 새로운 공급량 데이터가 저장된 열 이름\n",
    "        total_supply_col (str): 총 공급량 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 공급 변화율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['supply_change_rate'] = df[new_supply_col] / (df[total_supply_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multiple_rolling_features(df, exclude_columns_list):\n",
    "    \"\"\"\n",
    "    여러 윈도우 크기(6, 12, 24, 48)로 이동평균 피처를 생성하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    - df: 입력 데이터프레임\n",
    "    - exclude_columns_list: 이동평균을 적용하지 않을 열 이름이 담긴 리스트\n",
    "    \n",
    "    Returns:\n",
    "    - 이동평균 피처가 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    rolling_features = {}\n",
    "\n",
    "    # 윈도우 크기별로 변수명 리스트 생성\n",
    "    rolling_columns_dict = {window: [] for window in [6, 12, 24, 48]}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in exclude_columns_list:\n",
    "            for window in [6, 12, 24, 48]:\n",
    "                new_col_name = f'{col}_rolling_mean_{window}h'\n",
    "                rolling_features[new_col_name] = df[col].rolling(window=window).mean()\n",
    "                # 윈도우 크기에 맞는 리스트에 변수명 저장\n",
    "                rolling_columns_dict[window].append(new_col_name)\n",
    "\n",
    "    # 새로운 피처를 한 번에 추가\n",
    "    rolling_features_df = pd.DataFrame(rolling_features, index=df.index)\n",
    "    df = pd.concat([df, rolling_features_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 \n",
    "df = make_date_features(df, 'ID') # 날짜 피처\n",
    "df = make_diff_change_feature(df, ['open_interest']) # 변동성, 차분 피처 \n",
    "df = make_longshort_ratio_feature(df, 'long_liquidations', 'short_liquidations') # 롱/숏 비율\n",
    "df = make_liquidation_to_volume_ratio_feature(df, 'long_liquidations', 'short_liquidations', 'taker_buy_volume', 'taker_sell_volume') # 청산/거래량 비율\n",
    "df = make_liquidation_usd_ratio_feature(df, 'long_liquidations_usd', 'short_liquidations_usd') # 청산된 USD 롱/숏 비율\n",
    "df = make_funding_rate_position_change_feature(df, 'funding_rates', 'long_liquidations_usd', 'short_liquidations_usd') # 펀딩 비율과 롱/숏 포지션 차이 곱\n",
    "df = make_premium_diff_feature(df, 'coinbase_premium_gap', 'coinbase_premium_index') # 프리미엄 갭과 프리미엄 인덱스의 차이\n",
    "df = make_hashrate_to_difficulty_feature(df, 'hashrate', 'difficulty') # 해시레이트와 난이도 간의 비율\n",
    "df = make_supply_change_rate_feature(df, 'supply_new', 'supply_total') # 공급 변화율\n",
    "df = make_multiple_rolling_features(df, ['ID', 'target', '_type'] + ['year', 'month', 'week', 'day_of_week', 'hour'])\n",
    "\n",
    "rolling_columns = price_columns = [col for col in df.columns if 'rolling' in col]\n",
    "conti_cols = conti_cols + ['open_interest_pct_change', 'open_interest_diff', 'long_short_ratio', 'liquidation_to_volume_ratio', 'liquidation_usd_ratio', 'funding_rate_position_change', 'premium_diff', 'hashrate_to_difficulty', 'supply_change_rate'] + rolling_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/var/folders/4t/dyhc70gd7zjdwxdh7x7f77240000gn/T/ipykernel_4283/4175310149.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n"
     ]
    }
   ],
   "source": [
    "# 나머지 위의 피처엔지니어링 함수들을 실행한다.\n",
    "\n",
    "conti_cols = list(set(conti_cols) - set(rolling_columns))\n",
    "\n",
    "# 모든 수치형 컬럼에 대한 지수이동평균 계산해서 새로운 열로 할당\n",
    "for c in conti_cols:\n",
    "    df[c+\"_moving_avg_7\"] = EMA(df, c, 7)\n",
    "\n",
    "# 모든 수치형 컬럼에 대한 Wavelet transform을 계산해서 새로운 열로 할당\n",
    "for c in conti_cols:\n",
    "    df[c+\"WT\"] = WT(df, c)\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "df = df.drop(columns=new_features)\n",
    "\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yusolcho/opt/anaconda3/envs/datawrg/lib/python3.8/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.4434931506849315, auroc: 0.6441412344399869\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# lgb dataset\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "valid_data = lgb.Dataset(x_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "# lgb params (베이스라인 코드의 기본 파라미터임)\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 4,\n",
    "    \"num_leaves\": 50,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 30,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "# lgb train\n",
    "lgb_model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    "    valid_sets=valid_data,\n",
    ")\n",
    "\n",
    "# lgb predict\n",
    "y_valid_pred = lgb_model.predict(x_valid)\n",
    "y_valid_pred_class = np.argmax(y_valid_pred, axis = 1)\n",
    "\n",
    "# score check\n",
    "accuracy = accuracy_score(y_valid, y_valid_pred_class)\n",
    "auroc = roc_auc_score(y_valid, y_valid_pred, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"acc: {accuracy}, auroc: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yusolcho/opt/anaconda3/envs/datawrg/lib/python3.8/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "# performance 체크후 전체 학습 데이터로 다시 재학습\n",
    "x_train = train_df.drop([\"target\", \"ID\"], axis = 1)\n",
    "y_train = train_df[\"target\"].astype(int)\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "lgb_model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lgb_model.predict(test_df.drop([\"target\", \"ID\"], axis = 1))\n",
    "y_test_pred_class = np.argmax(y_test_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output File Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.assign(target = y_test_pred_class)\n",
    "submission.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('datawrg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2bf47c83e23e79da7154310486cd6f2111092cec5daef28d72dd2b3b6f44d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
