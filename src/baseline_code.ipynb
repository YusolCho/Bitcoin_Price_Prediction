{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 중에서 안깔린 라이브러리가 있을 수 있음\n",
    "\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/\"\n",
    "\n",
    "train = pd.read_csv(path+\"train.csv\").assign(_type=\"train\")\n",
    "test = pd.read_csv(path+\"test.csv\").assign(_type=\"test\")\n",
    "submission = pd.read_csv(path+\"test.csv\")\n",
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 원래 베이스라인 코드 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "# file_names: List[str] = [\n",
    "#     f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "# ]\n",
    "\n",
    "# # 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "# file_dict: Dict[str, pd.DataFrame] = {\n",
    "#     f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "# }\n",
    "\n",
    "# for _file_name, _df in tqdm(file_dict.items()):\n",
    "#     # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "#     _rename_rule = {\n",
    "#         col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "#         for col in _df.columns\n",
    "#     }\n",
    "#     _df = _df.rename(_rename_rule, axis=1)\n",
    "#     df = df.merge(_df, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 밑의 한 셀이 바꾼 부분 (이 바꾼 부분을 전제로 뒤의 코드가 실행됨.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOURLY_NETWORK-DATA_TRANSACTIONS-COUNT.csv',\n",
       " 'HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-INTERVAL.csv',\n",
       " 'HOURLY_NETWORK-DATA_ADDRESSES-COUNT.csv',\n",
       " 'HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX.csv',\n",
       " 'HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_NETWORK-DATA_HASHRATE.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCKREWARD.csv',\n",
       " 'HOURLY_NETWORK-DATA_SUPPLY.csv',\n",
       " 'HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE.csv',\n",
       " 'HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_UTXO-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-BYTES.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES-TRANSACTION.csv',\n",
       " 'HOURLY_NETWORK-DATA_TOKENS-TRANSFERRED.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES.csv',\n",
       " 'HOURLY_NETWORK-DATA_VELOCITY.csv',\n",
       " 'HOURLY_NETWORK-DATA_DIFFICULTY.csv',\n",
       " 'HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 31.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_로 시작하는 모든 csv 파일 불러오기\n",
    "prefixes = [\"HOURLY_\"]\n",
    "\n",
    "file_names = [\n",
    "    f for f in os.listdir(path)\n",
    "    if any(f.startswith(prefix) for prefix in prefixes) and f.endswith(\".csv\")\n",
    "]\n",
    "display(file_names)\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(path+f) for f in file_names\n",
    "}\n",
    "\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    _rename_rule = {\n",
    "        col: f\"{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결측치 (아거도 추가된 부분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transactions_count_total          4\n",
       "transactions_count_mean          28\n",
       "taker_buy_volume                 31\n",
       "taker_sell_volume                31\n",
       "taker_buy_ratio                  31\n",
       "taker_sell_ratio                 31\n",
       "taker_buy_sell_ratio             31\n",
       "block_interval                   24\n",
       "addresses_count_active            0\n",
       "addresses_count_sender            0\n",
       "addresses_count_receiver          0\n",
       "coinbase_premium_gap              6\n",
       "coinbase_premium_index            6\n",
       "open_interest                     4\n",
       "hashrate                          0\n",
       "blockreward                       0\n",
       "blockreward_usd                   0\n",
       "supply_total                     28\n",
       "supply_new                       28\n",
       "funding_rates                    32\n",
       "close                          2792\n",
       "volume                         2792\n",
       "block_count                       4\n",
       "utxo_count                        1\n",
       "block_bytes                      54\n",
       "fees_transaction_mean            24\n",
       "fees_transaction_mean_usd        24\n",
       "fees_transaction_median           0\n",
       "fees_transaction_median_usd       0\n",
       "tokens_transferred_total          0\n",
       "tokens_transferred_mean          24\n",
       "tokens_transferred_median         0\n",
       "fees_block_mean                  24\n",
       "fees_block_mean_usd              24\n",
       "fees_total                        0\n",
       "fees_total_usd                    0\n",
       "fees_reward_percent              24\n",
       "velocity_supply_total             0\n",
       "difficulty                       24\n",
       "long_liquidations                 0\n",
       "short_liquidations                0\n",
       "long_liquidations_usd             0\n",
       "short_liquidations_usd            0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "transactions_count_total       0.000346\n",
       "transactions_count_mean        0.002424\n",
       "taker_buy_volume               0.002684\n",
       "taker_sell_volume              0.002684\n",
       "taker_buy_ratio                0.002684\n",
       "taker_sell_ratio               0.002684\n",
       "taker_buy_sell_ratio           0.002684\n",
       "block_interval                 0.002078\n",
       "addresses_count_active         0.000000\n",
       "addresses_count_sender         0.000000\n",
       "addresses_count_receiver       0.000000\n",
       "coinbase_premium_gap           0.000519\n",
       "coinbase_premium_index         0.000519\n",
       "open_interest                  0.000346\n",
       "hashrate                       0.000000\n",
       "blockreward                    0.000000\n",
       "blockreward_usd                0.000000\n",
       "supply_total                   0.002424\n",
       "supply_new                     0.002424\n",
       "funding_rates                  0.002770\n",
       "close                          0.241690\n",
       "volume                         0.241690\n",
       "block_count                    0.000346\n",
       "utxo_count                     0.000087\n",
       "block_bytes                    0.004675\n",
       "fees_transaction_mean          0.002078\n",
       "fees_transaction_mean_usd      0.002078\n",
       "fees_transaction_median        0.000000\n",
       "fees_transaction_median_usd    0.000000\n",
       "tokens_transferred_total       0.000000\n",
       "tokens_transferred_mean        0.002078\n",
       "tokens_transferred_median      0.000000\n",
       "fees_block_mean                0.002078\n",
       "fees_block_mean_usd            0.002078\n",
       "fees_total                     0.000000\n",
       "fees_total_usd                 0.000000\n",
       "fees_reward_percent            0.002078\n",
       "velocity_supply_total          0.000000\n",
       "difficulty                     0.002078\n",
       "long_liquidations              0.000000\n",
       "short_liquidations             0.000000\n",
       "long_liquidations_usd          0.000000\n",
       "short_liquidations_usd         0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_new = df.drop([\"ID\", \"target\", \"_type\"], axis = 1)\n",
    "\n",
    "# 결측치 확인\n",
    "missing_data = df_new.isnull().sum()\n",
    "\n",
    "# 결측치 비율 확인\n",
    "missing_ratio = df_new.isnull().mean()\n",
    "\n",
    "\n",
    "display(missing_data.sort_values(ascending=False))\n",
    "display(missing_ratio.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions_count_total       0\n",
      "transactions_count_mean        0\n",
      "taker_buy_volume               0\n",
      "taker_sell_volume              0\n",
      "taker_buy_ratio                0\n",
      "taker_sell_ratio               0\n",
      "taker_buy_sell_ratio           0\n",
      "block_interval                 0\n",
      "addresses_count_active         0\n",
      "addresses_count_sender         0\n",
      "addresses_count_receiver       0\n",
      "coinbase_premium_gap           0\n",
      "coinbase_premium_index         0\n",
      "open_interest                  0\n",
      "hashrate                       0\n",
      "blockreward                    0\n",
      "blockreward_usd                0\n",
      "supply_total                   0\n",
      "supply_new                     0\n",
      "funding_rates                  0\n",
      "close                          0\n",
      "volume                         0\n",
      "block_count                    0\n",
      "utxo_count                     0\n",
      "block_bytes                    0\n",
      "fees_transaction_mean          0\n",
      "fees_transaction_mean_usd      0\n",
      "fees_transaction_median        0\n",
      "fees_transaction_median_usd    0\n",
      "tokens_transferred_total       0\n",
      "tokens_transferred_mean        0\n",
      "tokens_transferred_median      0\n",
      "fees_block_mean                0\n",
      "fees_block_mean_usd            0\n",
      "fees_total                     0\n",
      "fees_total_usd                 0\n",
      "fees_reward_percent            0\n",
      "velocity_supply_total          0\n",
      "difficulty                     0\n",
      "long_liquidations              0\n",
      "short_liquidations             0\n",
      "long_liquidations_usd          0\n",
      "short_liquidations_usd         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. 결측치 처리 - 앞/뒤 보간법 (forward/backward fill)\n",
    "\n",
    "# df_ffill = df_new.fillna(method='ffill')  # 앞의 값을 사용하여 결측치 채움\n",
    "# df_bfill = df_new.fillna(method='bfill')  # 뒤의 값을 사용하여 결측치 채움\n",
    "\n",
    "# 2. 결측치 처리 - 선형 보간법 (linear interpolation)\n",
    "df_interpolated = df_new.interpolate(method='linear')\n",
    "\n",
    "# 3. 결측치 처리 - 평균값으로 대체\n",
    "# df_mean_filled = df_new.fillna(eda_df.mean())\n",
    "\n",
    "df.update(df_interpolated)\n",
    "\n",
    "# 결측치 처리 후 확인\n",
    "print(df_interpolated.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 데이터를 로드할 때, (제공 받은)베이스라인 코드에서와 같이 거래소와 암호화폐 종류를 불러오도록 설정을 했다면\n",
    "\n",
    "아래 코드는 작동하지 않음\n",
    "\n",
    "거래소와 암호화폐 종류를 고려하지 않은 코드임을 유의.\n",
    "\n",
    "(수정을 해야할 것 같음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 독립변수에서 내가 사용할 독립변수 고르기\n",
    "\n",
    "\n",
    "# 전체 feature\n",
    "features = df.columns\n",
    "# 내가 쓸 feature\n",
    "fe = ['ID', 'target', '_type', 'funding_rates', 'open_interest',\n",
    "       'taker_buy_volume', 'taker_sell_volume', 'taker_buy_ratio',\n",
    "       'taker_sell_ratio', 'taker_buy_sell_ratio', 'difficulty',\n",
    "       'transactions_count_total', 'transactions_count_mean', 'block_count',\n",
    "       'fees_transaction_mean', 'fees_transaction_mean_usd',\n",
    "       'fees_transaction_median', 'fees_transaction_median_usd',\n",
    "       'fees_block_mean', 'fees_block_mean_usd', 'fees_total',\n",
    "       'fees_total_usd', 'fees_reward_percent', 'hashrate', 'utxo_count',\n",
    "       'tokens_transferred_total', 'tokens_transferred_mean',\n",
    "       'tokens_transferred_median', 'block_interval', 'velocity_supply_total',\n",
    "       'supply_total', 'supply_new', 'addresses_count_active',\n",
    "       'addresses_count_sender', 'addresses_count_receiver',\n",
    "       'blockreward', 'blockreward_usd', 'buy_sell_volume_ratio']\n",
    "\n",
    "# new_features에는 내가 사용하지 않을 feature들이 담기고 나중에 drop됨. 때문에, 이름이 좀 잘못된 듯\n",
    "new_features = [x for x in features if x not in fe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"taker_buy_volume\"] / (df[\"taker_sell_volume\"] + 1),\n",
    ")\n",
    "\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
    "conti_cols = [_ for _ in (df.columns)[3:] if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\",\n",
    "    \"liquidation_diff\",\n",
    "    \"liquidation_usd_diff\",\n",
    "    \"volume_diff\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    연속형 변수의 shift feature 생성\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    return df_shift_dict\n",
    "\n",
    "# 지수 이동 평균\n",
    "def EMA(df, col, span=2):\n",
    "    return df[col].ewm(span=span).mean()\n",
    "\n",
    "# Wavlet Transform\n",
    "def WT(df, col, wavelet='db5', th=0.6):\n",
    "    signal = df[col].values\n",
    "    th = th*np.nanmax(signal)\n",
    "    coef = pywt.wavedec(signal, wavelet, mode=\"per\" )\n",
    "    coef[1:] = (pywt.threshold(i, value=th, mode=\"soft\" ) for i in coef[1:])\n",
    "    reconstructed = pywt.waverec(coef, wavelet, mode=\"per\" )\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_253044/201995820.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n"
     ]
    }
   ],
   "source": [
    "# 위의 함수들을 실행한다.\n",
    "\n",
    "# 모든 수치형 컬럼에 대한 지수이동평균 계산해서 새로운 열로 할당\n",
    "for c in conti_cols:\n",
    "    df[c+\"_moving_avg_7\"] = EMA(df, c, 7)\n",
    "\n",
    "# 모든 수치형 컬럼에 대한 Wavelet transform을 계산해서 새로운 열로 할당\n",
    "for c in conti_cols:\n",
    "    df[c+\"WT\"] = WT(df, c)\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "df = df.drop(columns=new_features)\n",
    "\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml-lecture/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning:\n",
      "\n",
      "Found `n_estimators` in params. Will use it instead of argument\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.4223744292237443, auroc: 0.6145133857950079\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# lgb dataset\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "valid_data = lgb.Dataset(x_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "# lgb params (베이스라인 코드의 기본 파라미터임)\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 4,\n",
    "    \"num_leaves\": 50,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 30,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "# lgb train\n",
    "lgb_model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    "    valid_sets=valid_data,\n",
    ")\n",
    "\n",
    "# lgb predict\n",
    "y_valid_pred = lgb_model.predict(x_valid)\n",
    "y_valid_pred_class = np.argmax(y_valid_pred, axis = 1)\n",
    "\n",
    "# score check\n",
    "accuracy = accuracy_score(y_valid, y_valid_pred_class)\n",
    "auroc = roc_auc_score(y_valid, y_valid_pred, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"acc: {accuracy}, auroc: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "# performance 체크후 전체 학습 데이터로 다시 재학습\n",
    "x_train = train_df.drop([\"target\", \"ID\"], axis = 1)\n",
    "y_train = train_df[\"target\"].astype(int)\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "lgb_model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lgb_model.predict(test_df.drop([\"target\", \"ID\"], axis = 1))\n",
    "y_test_pred_class = np.argmax(y_test_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output File Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.assign(target = y_test_pred_class)\n",
    "submission.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
